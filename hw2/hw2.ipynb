{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"cellView":"form","id":"u3ku6Rrgxz-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638875488755,"user_tz":-120,"elapsed":17944,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"5e530459-918a-4aa8-87e4-a769d37147cc"},"source":["#@title ## Mount Your Google Drive\n","\n","#@markdown The next two cells are **magic** cells.\n","#@markdown They look like text cells, but they run code behind the scenes.\n","#@markdown You can run them by either clicking on the ‚ñ∂Ô∏è button (to the left of the cell), or by clicking on the cell and typing `Ctrl+Enter` (or `Shift+Enter`).\n","\n","#@markdown Please run this cell and follow the steps printed after running it. Specifically, it will print a URL you should enter, follow the instructions there and paste the code in the textbox below (and type `Enter`).\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"5iG-fCsoqvhu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638875502113,"user_tz":-120,"elapsed":801,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"137aa97e-d78c-4567-99dd-9a73eeae5c7b"},"source":["#@title ## Map Your Directory\n","import os\n","\n","def check_assignment(assignment_dir, files_list):\n","  files_in_dir = set(os.listdir(assignment_dir))\n","  for fname in files_list:\n","    if fname not in files_in_dir:\n","      raise FileNotFoundError(f'could not find file: {fname} in assignment_dir')\n","\n","assignment_dest = \"/content/hw2\"\n","assignment_dir = \"/content/gdrive/MyDrive/DL4CV/hw2\"  #@param{type:\"string\"}\n","assignment_files = ['hw2.ipynb', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n","                    'models.py', 'models_torch.py', 'train.py', 'train_torch.py', 'utils.py',\n","                    'test_autograd.py', 'test_functional.py', 'test_nn.py', 'test_optim.py']\n","\n","# check Google Drive is mounted\n","if not os.path.isdir(\"/content/gdrive\"):\n","  raise FileNotFoundError(\"Your Google Drive isn't mounted. Please run the above cell.\")\n","\n","# check all files there\n","check_assignment(assignment_dir, assignment_files)\n","\n","# create symbolic link\n","!rm -f {assignment_dest}\n","!ln -s \"{assignment_dir}\" \"{assignment_dest}\"\n","print(f'Succesfully mapped (ln -s) \"{assignment_dest}\" -> \"{assignment_dir}\"')\n","\n","# cd to linked dir\n","%cd -q {assignment_dest}\n","print(f'Succesfully changed directory (cd) to \"{assignment_dest}\"')\n","#@markdown Set the path `assignment_dir` to the assignment directory in your Google Drive and run this cell.\n","\n","#@markdown If you are not sure what is the path, you can use the **Files (üìÅ)** menu (on the left side) to check the path."],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Succesfully mapped (ln -s) \"/content/hw2\" -> \"/content/gdrive/MyDrive/DL4CV/hw2\"\n","Succesfully changed directory (cd) to \"/content/hw2\"\n"]}]},{"cell_type":"markdown","metadata":{"id":"pIfpUXsh9qmW"},"source":["## Imports and `autoreload`-Magic\n","Please run the cell below (only once) to load and set the `autoreload` magic, which automatically reloads the import calls to the python files with your solutions. That means that you can edit the files (in the right-side window), save them (`Ctrl+S`) and just re-run the relevant cells -- the new code will kick in automatically.\n","\n","**Note:** You **MUST NOT** install any package. If you can't load something, you probably didn't follow the instructions (either didn't uploaded all the files, didn't mounted your Google driver or didn't mapped your directory).\n","\n","**Note:** The exercise works as is. If you add or modify imports to things, it may break thing in the notebook. You may do so **AT YOUR OWN RISK**. We will not assist with issues in notebook with modified imports.\n","\n","**Note:** Make sure you run **all the cells** up to the point. Some cells depends on previous cells (mainly imports). Furthermore, make sure to run the cell below (with the autoreload magic) before any cell below it."]},{"cell_type":"code","metadata":{"id":"x_Xg1LYZlnQ9"},"source":["import torch\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rwfbzEQnoXr"},"source":["# (A) Implement Components for Deep Neural Network From Scratch\n","\n","In This part you will implement a deep neural network from scratch, including the necessary building blocks. You will implement it in the following order:\n","\n","1. **Differentiable Functions:** a set of differentiable functions that are used as atomic building blocks.\n","2. **Autograd's backward:** the back-propagation `backward` method.\n","3. **Learnable Layers:** the Linear layer.\n","4. **Optimizer:** the SGD optimizer which will be used for training.\n"]},{"cell_type":"markdown","metadata":{"id":"u32VOx2k3Pb8"},"source":["## (A.1) Differentiable Functions\n","\n","In this section you will implement a set of differentiable functions from scratch. For each function, you will implement the forward and backward methods. After the description of the method, there is a testing cell which we will test the correctness of your code.\n","\n","The skeletons of the differential functions to implement are in the `functional.py` file. Open this file by clicking on this link: `/content/hw2/functional.py`. Alternatively, you can go the left menu, click on **Files (üìÅ)**, go to the directory `hw2` (or `content/hw2`) and double-click on `functional.py` to open it. The tests can be found in `test_functional.py` (link: `/content/hw2/test_functional.py`).\n","\n","In each step you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","### `ctx`\n","In the \"from scratch\" implementation, you should use a `ctx` (context) variable. This variable is a simplified version of the computation graph, and is needed for the back propagation algorithm.\n","\n","Specifically, `ctx` is just a list (or stack) of \"backward calls\", where each \"backward call\" is a pair (list/tuple) of two objects:\n","\n","1. **`backward_fn`:** The backward function. A reference to the backward function to be called in the backward pass.\n","2. **`args`:** A list (or tuple) of arguments to be passed to `backward_fn`. This list usually consists of the inputs and the outputs of the forward function. Sometimes additional arguments are passed as well. It's important to pass the actual inputs and outputs (same pointer), otherwise it would break the chain of gradients propagation.\n","\n","The \"backward calls\" in `ctx` should be ordered in according to the time of addition. That is, a backward call that was added later should have an higher index in the list `ctx`. If `ctx` is `None`, it means that gradients (i.e. backward calls) should not be tracked.\n","\n","You will use `ctx` in the backward pass in section (A.2). You can read it now to get a little context (pun intended).\n","\n","**Note:** You are given an example of the forward and backward implementation of `mean`. You should read and understand how new backward calls are appended to `ctx`, and use this pattern in your solutions.\n","\n","**Note:** When new tensors are created (using `zeros`, `ones`, `rand`, etc.), it's important to make sure they are on the same device (and has the correct `dtype`) as tensors they would be used together with (compared to, multiplied by, etc.). You may find the functions `torch.X_like` and `Tensor.new_X` handy."]},{"cell_type":"markdown","metadata":{"id":"KxRhfmjXODEo"},"source":["### (A.1.1) Implement the Linear Function\n","\n","Here you will implement a differentiable `linear` function. This includes the forward `linear` function and the backward `linear_backward` function.\n","\n","#### `linear`\n","The `linear` function receives three arguments (in addition to the autograd context `ctx`):\n","\n","  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n","  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n","  * `b`: The bias term. Has shape `(out_dim,)`.\n","\n","It computes the (batched version of the) function: $$ \\mathbf{y} = W \\mathbf{x} + \\mathbf{b} $$\n","The output `y` should have shape `(batch_size, out_dim)`.\n","\n","#### `linear_backward`\n","The `linear_backward` function receives four arguments:\n","\n","  * `y`: The batched output. Has shape `(batch_size, out_dim)`.\n","  * `x`: The batched input. Has shape `(batch_size, in_dim)`.\n","  * `w`: The weight matrix. Has shape `(out_dim, in_dim)`.\n","  * `b`: The bias term. Has shape `(out_dim,)`.\n","\n","It computes the gradients of `x`, `w` and `b` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates these gradients in `x.grad`, `w.grad` and `b.grad`, respectively.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"id":"OkrZhq0U31I0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009173083,"user_tz":-120,"elapsed":3594,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"10578bf9-23b2-4dac-b160-7d305b6938bc"},"source":["!python -m unittest test_functional.TestLinear"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["......\n","----------------------------------------------------------------------\n","Ran 6 tests in 0.197s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"UAqdVJl9uxrI"},"source":["# Playground for debugging linear\n","from functional import linear, linear_backward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3vGFltjkDSm"},"source":["### (A.1.2) Implement the ReLU Activation\n","\n","Here you will implement a differentiable `relu` activation. This includes the forward `relu` function and the backward `relu_backward` function.\n","\n","#### `relu`\n","The `relu` function receives one argument (in addition to the autograd context `ctx`):\n","\n","  * `x`: The input. Has an arbitrary shape.\n","\n","It computes the (element-wise) function:\n","$$ y = \\max(x, 0) $$\n","The output `y` should have the same shape as `x`.\n","\n","#### `relu_backward`\n","The `relu_backward` function receives two arguments:\n","\n","  * `y`: The output. Has the same shape as `x`.\n","  * `x`: The input. Has an arbitrary shape.\n","\n","It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"id":"lA-NX0Y-j8yF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009175163,"user_tz":-120,"elapsed":2085,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"d4d43b74-d655-41f3-f2f4-7e1cce98292b"},"source":["!python -m unittest test_functional.TestReLU"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.008s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"m_79D4JGj888"},"source":["# Playground for debugging relu\n","from functional import relu, relu_backward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKtM0ZFakD3q"},"source":["### (A.1.3) Implement the Softmax Activation\n","\n","Here you will implement a differentiable `softmax` activation. This includes the forward `softmax` function and the backward `softmax_backward` function.\n","\n","**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n","\n","#### `softmax`\n","The `softmax` function receives one argument (in addition to the autograd context `ctx`):\n","\n","  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n","\n","It computes the (batched version of the) function: $$ \\mathbf{y}_i = \\frac{e^{\\mathbf{x}_i}}{\\sum_j{e^{\\mathbf{x}_j}}} $$\n","The output `y` should have the shape `(batch_size, num_classes)`. Each row in `y` should be a probability distribution over the classes.\n","\n","\n","#### `softmax_backward`\n","The `softmax_backward` function receives two arguments:\n","\n","  * `y`: The batched output. Has shape `(batch_size, num_classes)`.\n","  * `x`: The batched input. Has shape `(batch_size, num_classes)`.\n","\n","It computes the gradients of `x` w.r.t the loss, given the gradient of `y` (in `y.grad`) w.r.t the loss, and accumulates this gradient in `x.grad`.\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it."]},{"cell_type":"code","metadata":{"id":"AqSS59Pej9FY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009177146,"user_tz":-120,"elapsed":1992,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"22417a1c-2909-468e-fd56-ec807d9d9caa"},"source":["!python -m unittest test_functional.TestSoftmax"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.049s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"jHN0ZW5Xj9Lv"},"source":["# Playground for debugging softmax\n","from functional import softmax, softmax_backward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zL1_-cE5kEiG"},"source":["### (A.1.4) Implement the Cross-Entropy Loss\n","\n","Here you will implement a differentiable `cross_entropy` activation. This includes the forward `cross_entropy` function and the backward `cross_entropy_backward` function.\n","\n","**Note:** Similarly to homework assignment #1, your solution should be numerically stable.\n","\n","**Note:** The signature of this function differs from PyTorch's `F.cross_entropy`. The function you should implement doesn't \"reduce\" (i.e. averages over) the batch (similarly to `F.cross_entropy(..., reduction='none')`). Furthermore, while `F.cross_entropy` receives the predictions **before** `softmax`, the function you should implement receives the predictions **after** `softmax`. We provide you the `cross_entropy_loss` which uses your implementation of `softmax` and `cross_entropy`, and has the same API as `F.cross_entropy`.\n","\n","#### `cross_entropy`\n","The `cross_entropy` function receives two arguments (in addition to the autograd context `ctx`):\n","\n","  * `pred`: The predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n","  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n","\n","It computes the (batched version of the) function:\n","$$ \\text{CE}(\\hat{\\mathbf{y}}, \\ell)_i = -\\log(\\hat{\\mathbf{y}}_i) \\cdot \\delta_{i,\\ell} $$\n","Where $\\hat{\\mathbf{y}}$ (also called `pred` or `y_hat`) is the predicted probability measure over the classes and $\\ell$ (also called `target` or `y`) is the target class label.\n","\n","The output `loss` should have the shape `(batch_size,)`. Each row in `loss` should be the cross-entropy loss of that entry in the batch.\n","\n","#### `cross_entropy_backward`\n","The `cross_entropy_backward` function receives three arguments:\n","\n","  * `loss`: The batched loss. Has shape `(batch_size,)`.\n","  * `pred`: The batched predicted _probabilities_. Has shape `(batch_size, num_classes)`. Each row is a probability distribution (non-negative values; sums to 1).\n","  * `target`: The batched correct labels. Has type of `torch.long` (integer values), and has shape `(batch_size,)`. Its values are between `0` and `num_classes - 1` (inclusive).\n","\n","It computes the gradients of `pred` w.r.t the (final scalar) loss, given the gradient of (batched) `loss` (in `loss.grad`) w.r.t the loss, and accumulates this gradient in `pred.grad`.\n","\n","#### `cross_entropy_loss`\n","This function is provided for your use. It calls `softmax` to compute the probability distribution over the labels, then `cross_entropy` to computed the batched loss, and later `mean` to reduce it into a scalar loss (that can be used as the origin of gradients; see next part). You should NOT modify this method, and may use it later on.\n","\n","**Note:** Please see how three differentiable functions (`softmax`, `cross_entropy` and `mean`) are chained to create a new differentiable function, without explicitly implementing its backward pass. You will chain differentiable functions to create a model in section (B).\n","\n","---\n","You should test your solution by running the following cell. You can debug your solution in the cell below it.\n"]},{"cell_type":"code","metadata":{"id":"3jW24PjRj9RE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009179149,"user_tz":-120,"elapsed":2013,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"9a43f50d-0b46-4b92-cab0-14bf1b22fdae"},"source":["!python -m unittest test_functional.TestCrossEntropy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".......\n","----------------------------------------------------------------------\n","Ran 7 tests in 0.039s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"S6au2xEbj9Vu"},"source":["# Playground for debugging cross_entropy\n","from functional import cross_entropy, cross_entropy_backward\n","from functional import cross_entropy_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7e_g4fDakFoQ"},"source":["## (A.2) Autograd\n","\n","In this section you will implement a general `backward` method from scratch. This method stands at the core of back-propagation and autograd differentiation.\n","\n","This method receives two arguments:\n","\n","* `loss`: The loss tensor. This tensor must be a scalar (Has shape `()`). The loss the other tensors will be computed w.r.t this `loss`.\n","* `ctx`: The autograd context. A list of backward calls. These backward calls should be evaluated to back-propagate the gradient from `loss` to the tensors used in the computation of `loss`.\n","\n","This method has two main steps:\n","\n","* Setting the gradient of `loss` (to what?).\n","* Propagating the gradients backward using the computation history in `ctx` (how?).\n","\n","The skeleton of the `backward` method is in the `autograd.py` file (link: `/content/hw2/autograd.py`). The tests can be found in `test_autograd.py` (link: `/content/hw2/test_autograd.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. You can use the provided `create_grad_if_necessary` which makes sure that tensors that need gradients have one (if not, it creates a `.grad` attribute in the tensor's shape filled with zeros). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"]},{"cell_type":"code","metadata":{"id":"bTb1J0bFj9ag","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009182016,"user_tz":-120,"elapsed":2876,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"97db676d-8c7a-46b4-9aa7-54e58974656f"},"source":["!python -m unittest test_autograd.TestBackward"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["...\n","----------------------------------------------------------------------\n","Ran 3 tests in 0.014s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"veWDHxTTj9fl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009182017,"user_tz":-120,"elapsed":26,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"ce319151-e244-49c3-ef6f-8f9b0eb71e6d"},"source":["# Playground for debugging backward\n","from autograd import backward\n","from functional import softmax, cross_entropy_loss\n","\n","# You CAN modify the content of the cell below. It is just an example.\n","ctx = []\n","x = torch.randn(4, 5)\n","l = torch.randint(5, size=(4,), dtype=torch.long)\n","y = softmax(x, ctx=ctx)\n","loss = cross_entropy_loss(y, l, ctx=ctx)\n","\n","print('before backward')\n","print('loss.grad:', loss.grad)\n","print('y.grad:', y.grad)\n","print('x.grad:', x.grad)\n","\n","backward(loss, ctx)\n","\n","print('\\n\\nafter backward')\n","print('loss.grad:', loss.grad)\n","print('y.grad:', y.grad)\n","print('x.grad:', x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["before backward\n","loss.grad: None\n","y.grad: None\n","x.grad: None\n","\n","\n","after backward\n","loss.grad: tensor(1.)\n","y.grad: tensor([[-0.2066,  0.0596,  0.0472,  0.0465,  0.0534],\n","        [-0.2082,  0.0429,  0.0751,  0.0428,  0.0474],\n","        [-0.2071,  0.0578,  0.0484,  0.0518,  0.0491],\n","        [-0.2049,  0.0418,  0.0423,  0.0581,  0.0627]])\n","x.grad: tensor([[-0.0158,  0.0086,  0.0015,  0.0013,  0.0044],\n","        [-0.0122, -0.0007,  0.0144, -0.0007, -0.0009],\n","        [-0.0129,  0.0065,  0.0016,  0.0030,  0.0018],\n","        [-0.0260,  0.0004,  0.0006,  0.0104,  0.0146]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"9F30SfhnmkWP"},"source":["## (A.3) Learnable Layers\n","\n","In this section you will implement a learnable Linear layer. The implementation is similar to vanilla PyTorch.\n","\n","The skeleton of the learnable Linear layer to implement is in the `nn.py` file (link: `/content/hw2/nn.py`). The tests can be found in `test_nn.py` (link: `/content/hw2/test_nn.py`).\n","\n","Learnable layers (and networks) inherits from the provided class `Module` (which is similar to PyTorch's `nn.Module`). This abstract class implements some utility methods (some are not used in this assignment). Please read the list of `Module`'s methods and attributes in its documentation (link: `/content/hw2/nn.py`).\n","\n","In the `nn.py` file, you should fill the blanks (between `# BEGIN SOLUTION` and `# END SOLUTION`) in the relevant methods. DO NOT change any other code segments. You are provided with a cell to run the tests, and with a cell to debug your code (with the relevant imports). As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","**Note:** To see how \"atomic\" differentiable functions are composed into a complex differentiable function, please look at the provided `cross_entropy_loss` in `/content/hw2/functional.py`.\n","\n","**Note:** Since this part doesn't use PyTorch's built-in autograd mechanism, please do not use tensors' `requires_grad` (this will result in errors/warnings).\n","Furthermore, do not use `nn.Parameter` in _from scratch_ layers."]},{"cell_type":"markdown","metadata":{"id":"6MkxNHgUnAoa"},"source":["### (A.3.1) Implement the Linear Layer\n","\n","So far you have implemented *stateless* differentiable functions, and the autograd mechanism. In this section, you will implement a *stateful* layer, with learnable parameters. That is the `Linear` layer.\n","\n","The parameters of the `Linear` layer are the weight matrix `weight` and the bias term `bias`. In your layer, you should:\n","\n","1. **Create parameter tensors:** create tensors for the parameters in the correct shape. The parameters should be attributes of the layer, i.e. set as `self.<param> = <tensor>`. This is done in `Linear.__init__`.\n","2. **Register them as parameters:** add their names to `self._parameters`. This will be used by the provided `Module.parameters()` (to list module's parameters) and `Module.to()` (to trasfer module's parameters to a device) methods. This is done in `Linear.__init__`.\n","3. **Initialize the parameters:** initialization of the layer parameters has significant influence on the local minimum the network reaches during training. This is done in `Linear.init_parameters()`. You should call this method from `Linear.__init__`, so newly created linear layers are initialized.\n","4. **Implement a forward method:** use the existing differentiable function from part A, and implement the `Linear.forward()` method."]},{"cell_type":"code","metadata":{"id":"ARMFodrXnBfW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009184336,"user_tz":-120,"elapsed":2341,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"bcaa3a31-8615-4ffd-d4dc-7bd373074a03"},"source":["!python -m unittest test_nn.TestLinear"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["....\n","----------------------------------------------------------------------\n","Ran 4 tests in 0.015s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"wENev5CtcjnT"},"source":["# Playground for debugging Linear\n","from nn import Linear"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTKP7Ovas-_M"},"source":["## (A.4) Optimizer\n","\n","In this section you will implement an optimizer. The optimizer updates the parameters based on the gradients they had accumulated. To do so it should have three main functions:\n","\n","1. `__init__`: Receives the list of parameters (weights) to update their values and save them. May receive additional arguments, such as learning-rate, etc.\n","2. `step`: Updates the parameters values based on the value of their gradients. Doesn't receive any argument.\n","3. `zero_grad`: Zeros the gradients of the tracked parameters. This is necessary since gradients are accumulated in each backward pass, and we don't want to mix between batches. Doesn't receive any argument.\n","\n","The skeleton of the optimizer is in the `optim.py` file (link: `/content/hw2/optim.py`). The tests can be found in `test_optim.py` (link: `/content/hw2/test_optim.py`). You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. DO NOT change any other code segments. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n"]},{"cell_type":"markdown","metadata":{"id":"PK5kaf5kmoxM"},"source":["### (A.4.1) SGD Optimizer\n","In this part, you'll implement an SGD optimizer. This optimizer has a simple update rule, which is:\n","$$\\mathbf{x}_{n+1} = \\mathbf{x}_{n} - \\delta \\cdot \\mathbf{g}_{n} $$\n","Where $\\mathbf{x}_{n}$ is the parameter at step $n$, $\\mathbf{g}_{n}$ is its gradient at step $n$, and $\\delta$ is the learning rate (also called `lr`).\n","\n","You should implement the `__init__`, `step` and `zero_grad` methods of `SGD` optimizer in `optim.py`.\n","\n","**Note:** Parameters (tensors) should be updated **in-place** (i.e. with the `-=` operator) in `step`.\n","\n","**Note:** A gradient (`param.grad`) which is set to `None` is also considered as zero."]},{"cell_type":"code","metadata":{"id":"iR4bRY16aW1Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009186876,"user_tz":-120,"elapsed":2551,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"4119627f-4598-4ae0-b3ab-32fd0dbdf447"},"source":["!python -m unittest test_optim.TestSGD"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["..\n","----------------------------------------------------------------------\n","Ran 2 tests in 0.010s\n","\n","OK\n"]}]},{"cell_type":"code","metadata":{"id":"dZlZwEqGj9_2"},"source":["# Playground for debugging SGD\n","from optim import SGD"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XStdhdlPcSBd"},"source":["# Setup Before Training\n","\n","In this part you will need to use GPU (this will have a significant impact on the training speed). To get a GPU in Google Colab, please go to the top menu and to: **Runtime ‚ûî Change runtime type**. Then, select **GPU** as **Hardware accelerator**.\n","\n","Please run the cell below to set your pytorch device (either GPU or CPU), to load the dataset and to create data loaders.\n","\n"]},{"cell_type":"code","metadata":{"id":"cVRcw-dlgFLb"},"source":["from utils import load_mnist\n","\n","# Set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","pin_memory = device.type == 'cuda'\n","\n","# Load the training and test sets\n","train_data = load_mnist(mode='train')\n","test_data = load_mnist(mode='test')\n","\n","# Create dataloaders for training and test sets\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, pin_memory=pin_memory)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, pin_memory=pin_memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GiJaEoARnAOn"},"source":["# (B) Define and Train Neural Networks From Scratch\n","\n","\n","In this part, you will define and train neural networks from scratch. You will use your differentiable functions from section (A).\n","\n","The skeletons for this assignment can be found in the `models.py` (link: `/content/hw2/models.py`) and `train.py` (link: `/content/hw2/train.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","Please run the cell below to import the relevant objects in order to train the models."]},{"cell_type":"code","metadata":{"id":"PzTMHw9yKxN6"},"source":["from functional import cross_entropy_loss as cross_entropy_scratch\n","from models import SoftmaxClassifier as SoftmaxClassifierScratch\n","from models import MLP as MLPScratch\n","from optim import SGD as SGDScratch\n","from train import train_loop as train_loop_scratch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW255-xmnA6O"},"source":["## (B.1) Implement and Train a SoftmaxClassifier\n","\n","Here you will implement the `SoftmaxClassifier` (imported here as `SoftmaxClassifierScratch`). You have already implemented the `SoftmaxClassifier` in Homework 1, but now it will be implemented with autograd and modular differentiable functions.\n","\n","Your solution should have the following parts:\n","\n","1. Create a model.\n","2. (Optional) Transfer the model to `device`.\n","3. Create an optimizer. (this should be done when the model is in its final device. It will not work otherwise).\n","4. Set other hyper-parameters (loss function, number of epochs, etc.).\n","5. Train the model.\n","\n","**Note:** As opposed to its name, `SoftmaxClassifier` should not perform softmax. That's because softmax part of the cross-entropy loss (in PyTorch and in the _from scratch_ section).\n"]},{"cell_type":"code","metadata":{"id":"EQlsopQc3dF5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009445254,"user_tz":-120,"elapsed":254266,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"a6f1a80e-f358-434a-cb0f-1b8a30cd33a2"},"source":["# BEGIN SOLUTION\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model\n","model = SoftmaxClassifierScratch(28*28,10)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","lr = 3e-4\n","# print(model.parameters())\n","optimizer = SGDScratch(model.parameters(),lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_scratch\n","\n","# Set the number of epochs\n","epochs = 20\n","\n","# Train your model\n","train_loop_scratch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train   Epoch: 001 / 020   Loss:    1.44   Accuracy: 0.732\n"," Test   Epoch: 001 / 020   Loss:  0.9683   Accuracy: 0.833\n","Train   Epoch: 002 / 020   Loss:  0.8393   Accuracy: 0.836\n"," Test   Epoch: 002 / 020   Loss:  0.7089   Accuracy: 0.857\n","Train   Epoch: 003 / 020   Loss:  0.6733   Accuracy: 0.855\n"," Test   Epoch: 003 / 020   Loss:  0.6017   Accuracy: 0.870\n","Train   Epoch: 004 / 020   Loss:  0.5929   Accuracy: 0.864\n"," Test   Epoch: 004 / 020   Loss:  0.5419   Accuracy: 0.876\n","Train   Epoch: 005 / 020   Loss:  0.5441   Accuracy: 0.870\n"," Test   Epoch: 005 / 020   Loss:  0.5028   Accuracy: 0.879\n","Train   Epoch: 006 / 020   Loss:  0.5108   Accuracy: 0.874\n"," Test   Epoch: 006 / 020   Loss:  0.4752   Accuracy: 0.884\n","Train   Epoch: 007 / 020   Loss:  0.4863   Accuracy: 0.877\n"," Test   Epoch: 007 / 020   Loss:  0.4543   Accuracy: 0.887\n","Train   Epoch: 008 / 020   Loss:  0.4674   Accuracy: 0.880\n"," Test   Epoch: 008 / 020   Loss:  0.4381   Accuracy: 0.889\n","Train   Epoch: 009 / 020   Loss:  0.4523   Accuracy: 0.883\n"," Test   Epoch: 009 / 020   Loss:  0.4249   Accuracy: 0.891\n","Train   Epoch: 010 / 020   Loss:  0.4398   Accuracy: 0.885\n"," Test   Epoch: 010 / 020   Loss:  0.4137   Accuracy: 0.892\n","Train   Epoch: 011 / 020   Loss:  0.4293   Accuracy: 0.887\n"," Test   Epoch: 011 / 020   Loss:  0.4045   Accuracy: 0.894\n","Train   Epoch: 012 / 020   Loss:  0.4203   Accuracy: 0.888\n"," Test   Epoch: 012 / 020   Loss:  0.3963   Accuracy: 0.896\n","Train   Epoch: 013 / 020   Loss:  0.4125   Accuracy: 0.890\n"," Test   Epoch: 013 / 020   Loss:  0.3894   Accuracy: 0.897\n","Train   Epoch: 014 / 020   Loss:  0.4057   Accuracy: 0.891\n"," Test   Epoch: 014 / 020   Loss:  0.3832   Accuracy: 0.899\n","Train   Epoch: 015 / 020   Loss:  0.3996   Accuracy: 0.892\n"," Test   Epoch: 015 / 020   Loss:  0.3777   Accuracy: 0.899\n","Train   Epoch: 016 / 020   Loss:  0.3941   Accuracy: 0.893\n"," Test   Epoch: 016 / 020   Loss:  0.3728   Accuracy: 0.900\n","Train   Epoch: 017 / 020   Loss:  0.3892   Accuracy: 0.894\n"," Test   Epoch: 017 / 020   Loss:  0.3684   Accuracy: 0.900\n","Train   Epoch: 018 / 020   Loss:  0.3847   Accuracy: 0.895\n"," Test   Epoch: 018 / 020   Loss:  0.3644   Accuracy: 0.901\n","Train   Epoch: 019 / 020   Loss:  0.3806   Accuracy: 0.896\n"," Test   Epoch: 019 / 020   Loss:  0.3606   Accuracy: 0.902\n","Train   Epoch: 020 / 020   Loss:  0.3768   Accuracy: 0.896\n"," Test   Epoch: 020 / 020   Loss:  0.3573   Accuracy: 0.903\n"]}]},{"cell_type":"markdown","metadata":{"id":"SNtYgm9vnGtR"},"source":["## (B.2) Implement and Train a Deep Neural Network\n","\n","Here you will implement a multi-layer perceptron (`MLP`) model (imported here as `MLPScratch`). You are allowed to modify the signiture of `MLP.__init__` and add additional arguments to your choice. Your network must have more than a single linear layer.\n","\n","Your solution should have the same parts as in (B.1)."]},{"cell_type":"code","metadata":{"id":"vVU5EM6CSgHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638009735450,"user_tz":-120,"elapsed":290209,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"fd94eee6-f14b-4a79-cfe3-6616f45e92fc"},"source":["# BEGIN SOLUTION\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model\n","model = MLPScratch(28*28,10,128,62,32)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","lr = 1e-1\n","optimizer = SGDScratch(model.parameters(),lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_scratch\n","\n","# Set the number of epochs\n","epochs = 20\n","\n","# Train your model\n","train_loop_scratch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train   Epoch: 001 / 020   Loss:  0.6176   Accuracy: 0.791\n"," Test   Epoch: 001 / 020   Loss:  0.1892   Accuracy: 0.941\n","Train   Epoch: 002 / 020   Loss:  0.1481   Accuracy: 0.956\n"," Test   Epoch: 002 / 020   Loss:  0.1428   Accuracy: 0.957\n","Train   Epoch: 003 / 020   Loss: 0.09695   Accuracy: 0.971\n"," Test   Epoch: 003 / 020   Loss:  0.1265   Accuracy: 0.960\n","Train   Epoch: 004 / 020   Loss: 0.07243   Accuracy: 0.978\n"," Test   Epoch: 004 / 020   Loss: 0.09344   Accuracy: 0.973\n","Train   Epoch: 005 / 020   Loss:  0.0561   Accuracy: 0.983\n"," Test   Epoch: 005 / 020   Loss: 0.08856   Accuracy: 0.976\n","Train   Epoch: 006 / 020   Loss: 0.04514   Accuracy: 0.987\n"," Test   Epoch: 006 / 020   Loss: 0.08712   Accuracy: 0.975\n","Train   Epoch: 007 / 020   Loss: 0.03636   Accuracy: 0.989\n"," Test   Epoch: 007 / 020   Loss:  0.1513   Accuracy: 0.954\n","Train   Epoch: 008 / 020   Loss: 0.02933   Accuracy: 0.991\n"," Test   Epoch: 008 / 020   Loss:  0.1027   Accuracy: 0.973\n","Train   Epoch: 009 / 020   Loss: 0.02591   Accuracy: 0.992\n"," Test   Epoch: 009 / 020   Loss: 0.09587   Accuracy: 0.975\n","Train   Epoch: 010 / 020   Loss: 0.02183   Accuracy: 0.993\n"," Test   Epoch: 010 / 020   Loss:  0.1077   Accuracy: 0.973\n","Train   Epoch: 011 / 020   Loss: 0.01778   Accuracy: 0.994\n"," Test   Epoch: 011 / 020   Loss:  0.1026   Accuracy: 0.975\n","Train   Epoch: 012 / 020   Loss: 0.01407   Accuracy: 0.996\n"," Test   Epoch: 012 / 020   Loss:  0.1016   Accuracy: 0.977\n","Train   Epoch: 013 / 020   Loss: 0.01146   Accuracy: 0.997\n"," Test   Epoch: 013 / 020   Loss: 0.09796   Accuracy: 0.979\n","Train   Epoch: 014 / 020   Loss: 0.01094   Accuracy: 0.997\n"," Test   Epoch: 014 / 020   Loss: 0.08961   Accuracy: 0.979\n","Train   Epoch: 015 / 020   Loss: 0.009103   Accuracy: 0.997\n"," Test   Epoch: 015 / 020   Loss:  0.1067   Accuracy: 0.976\n","Train   Epoch: 016 / 020   Loss: 0.006614   Accuracy: 0.998\n"," Test   Epoch: 016 / 020   Loss:  0.1102   Accuracy: 0.978\n","Train   Epoch: 017 / 020   Loss: 0.007864   Accuracy: 0.997\n"," Test   Epoch: 017 / 020   Loss:  0.1206   Accuracy: 0.975\n","Train   Epoch: 018 / 020   Loss: 0.008511   Accuracy: 0.998\n"," Test   Epoch: 018 / 020   Loss:  0.1052   Accuracy: 0.977\n","Train   Epoch: 019 / 020   Loss: 0.01047   Accuracy: 0.997\n"," Test   Epoch: 019 / 020   Loss:  0.1143   Accuracy: 0.979\n","Train   Epoch: 020 / 020   Loss: 0.009856   Accuracy: 0.997\n"," Test   Epoch: 020 / 020   Loss:  0.1294   Accuracy: 0.975\n"]}]},{"cell_type":"markdown","metadata":{"id":"2PSeMqMC3fBc"},"source":["# (C) Define and Train PyTorch Neural Networks\n","\n","In this part, you will define and train neural networks using PyTorch's built-in autograd mechanism. You MAY NOT use your differentiable functions from section (A). The solution to this part is very similar to the solution of part (B), with some syntax changes.\n","\n","The skeletons for this assignment can be found in the `models_torch.py` (link: `/content/hw2/models_torch.py`) and `train_torch.py` (link: `/content/hw2/train_torch.py`) files. You should fill the blanks between `# BEGIN SOLUTION` and `# END SOLUTION`. As a reminder, this notebook uses the `autoreload` magic which automatically reloads the imported `.py` files (just make sure you save these file with `Ctrl+S`).\n","\n","Please run the cell below to import the relevant objects in order to train the models.\n","\n","**Note:** some methods are imported with different names in this notebook to distinguish them from the _From Scratch_ part. This is not a best practice, and used solely as a way to avoid ambiguities in this assignment."]},{"cell_type":"code","metadata":{"id":"1eMq2E6F3kwR"},"source":["# NOTE: `cross_entropy_torch` is different from `cross_entropy_scratch`!\n","# cross_entropy_torch(pred, target) == cross_entropy_scratch(softmax(pred), target)\n","from torch.nn.functional import cross_entropy as cross_entropy_torch\n","from models_torch import SoftmaxClassifier as SoftmaxClassifierTorch\n","from models_torch import MLP as MLPTorch\n","from torch.optim import SGD as SGDTorch\n","from train_torch import train_loop as train_loop_torch\n","from utils import load_mnist"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFI-VKr73bcE"},"source":["## (C.1) Implement and Train a Softmax Classifier\n","\n","Here you will implement the `SoftmaxClassifier` class (imported as `SoftmaxClassifierTorch`).\n","\n","Your solution should have the same parts as in (B.1)."]},{"cell_type":"code","metadata":{"id":"e0U2HGz13b0W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638011306856,"user_tz":-120,"elapsed":242867,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"b3732ab5-9e59-40bb-cccb-e484a3c83f70"},"source":["# BEGIN SOLUTION\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model\n","model = SoftmaxClassifierTorch(28*28,10)\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","lr = 3e-2\n","# print(model.parameters())\n","optimizer = SGDTorch(model.parameters(),lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_torch\n","\n","# Set the number of epochs\n","epochs = 20\n","\n","# Train your model\n","train_loop_torch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train   Epoch: 001 / 020   Loss:  0.3846   Accuracy: 0.890\n"," Test   Epoch: 001 / 020   Loss:  0.2999   Accuracy: 0.916\n","Train   Epoch: 002 / 020   Loss:  0.3008   Accuracy: 0.914\n"," Test   Epoch: 002 / 020   Loss:  0.2832   Accuracy: 0.920\n","Train   Epoch: 003 / 020   Loss:  0.2864   Accuracy: 0.919\n"," Test   Epoch: 003 / 020   Loss:  0.2771   Accuracy: 0.921\n","Train   Epoch: 004 / 020   Loss:  0.2779   Accuracy: 0.921\n"," Test   Epoch: 004 / 020   Loss:  0.2732   Accuracy: 0.921\n","Train   Epoch: 005 / 020   Loss:  0.2712   Accuracy: 0.922\n"," Test   Epoch: 005 / 020   Loss:  0.2712   Accuracy: 0.921\n","Train   Epoch: 006 / 020   Loss:  0.2663   Accuracy: 0.924\n"," Test   Epoch: 006 / 020   Loss:  0.2714   Accuracy: 0.922\n","Train   Epoch: 007 / 020   Loss:   0.263   Accuracy: 0.925\n"," Test   Epoch: 007 / 020   Loss:  0.2655   Accuracy: 0.924\n","Train   Epoch: 008 / 020   Loss:  0.2594   Accuracy: 0.927\n"," Test   Epoch: 008 / 020   Loss:  0.2683   Accuracy: 0.922\n","Train   Epoch: 009 / 020   Loss:  0.2566   Accuracy: 0.928\n"," Test   Epoch: 009 / 020   Loss:  0.2686   Accuracy: 0.921\n","Train   Epoch: 010 / 020   Loss:  0.2543   Accuracy: 0.929\n"," Test   Epoch: 010 / 020   Loss:  0.2619   Accuracy: 0.925\n","Train   Epoch: 011 / 020   Loss:  0.2525   Accuracy: 0.929\n"," Test   Epoch: 011 / 020   Loss:  0.2653   Accuracy: 0.924\n","Train   Epoch: 012 / 020   Loss:  0.2505   Accuracy: 0.929\n"," Test   Epoch: 012 / 020   Loss:  0.2569   Accuracy: 0.926\n","Train   Epoch: 013 / 020   Loss:  0.2491   Accuracy: 0.930\n"," Test   Epoch: 013 / 020   Loss:  0.2607   Accuracy: 0.925\n","Train   Epoch: 014 / 020   Loss:  0.2474   Accuracy: 0.930\n"," Test   Epoch: 014 / 020   Loss:  0.2608   Accuracy: 0.926\n","Train   Epoch: 015 / 020   Loss:  0.2463   Accuracy: 0.930\n"," Test   Epoch: 015 / 020   Loss:  0.2588   Accuracy: 0.925\n","Train   Epoch: 016 / 020   Loss:  0.2449   Accuracy: 0.930\n"," Test   Epoch: 016 / 020   Loss:  0.2609   Accuracy: 0.925\n","Train   Epoch: 017 / 020   Loss:  0.2435   Accuracy: 0.932\n"," Test   Epoch: 017 / 020   Loss:  0.2565   Accuracy: 0.927\n","Train   Epoch: 018 / 020   Loss:  0.2427   Accuracy: 0.932\n"," Test   Epoch: 018 / 020   Loss:  0.2549   Accuracy: 0.927\n","Train   Epoch: 019 / 020   Loss:  0.2418   Accuracy: 0.932\n"," Test   Epoch: 019 / 020   Loss:    0.26   Accuracy: 0.925\n","Train   Epoch: 020 / 020   Loss:  0.2404   Accuracy: 0.932\n"," Test   Epoch: 020 / 020   Loss:  0.2602   Accuracy: 0.924\n"]}]},{"cell_type":"markdown","metadata":{"id":"Gc3NLJiwXsuH"},"source":["## (C.2) Implement and Train a Deep Neural Network\n","\n","Here you will implement the `MLP` class (imported as `MLPTorch`).\n","\n","Your solution should have the same parts as in (B.2)."]},{"cell_type":"code","metadata":{"id":"JbXH5Dt4LVHu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638010793394,"user_tz":-120,"elapsed":267170,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"bf5b79e1-e007-4d35-f832-e1898ad58ca1"},"source":["# BEGIN SOLUTION\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define your model\n","model = MLPTorch(28*28,10,[128,64,32])\n","\n","# Transfer it to device\n","model = model.to(device)\n","\n","# Set an optimizer\n","lr = 9e-2\n","# print(model.parameters())\n","optimizer = SGDTorch(model.parameters(),lr)\n","\n","# Set a criterion (loss function)\n","criterion = cross_entropy_torch\n","\n","# Set the number of epochs\n","epochs = 20\n","\n","# Train your model\n","train_loop_torch(model=model,\n","                   criterion=criterion,\n","                   optimizer=optimizer,\n","                   train_loader=train_loader,\n","                   test_loader=test_loader,\n","                   device=device,\n","                   epochs=epochs)\n","# END SOLUTION"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train   Epoch: 001 / 020   Loss:  0.6347   Accuracy: 0.799\n"," Test   Epoch: 001 / 020   Loss:  0.2489   Accuracy: 0.923\n","Train   Epoch: 002 / 020   Loss:  0.1345   Accuracy: 0.960\n"," Test   Epoch: 002 / 020   Loss:  0.1322   Accuracy: 0.960\n","Train   Epoch: 003 / 020   Loss: 0.09043   Accuracy: 0.973\n"," Test   Epoch: 003 / 020   Loss: 0.09456   Accuracy: 0.971\n","Train   Epoch: 004 / 020   Loss:  0.0684   Accuracy: 0.979\n"," Test   Epoch: 004 / 020   Loss: 0.08336   Accuracy: 0.974\n","Train   Epoch: 005 / 020   Loss: 0.05129   Accuracy: 0.984\n"," Test   Epoch: 005 / 020   Loss: 0.08525   Accuracy: 0.973\n","Train   Epoch: 006 / 020   Loss: 0.04123   Accuracy: 0.987\n"," Test   Epoch: 006 / 020   Loss:  0.1017   Accuracy: 0.971\n","Train   Epoch: 007 / 020   Loss: 0.03389   Accuracy: 0.989\n"," Test   Epoch: 007 / 020   Loss: 0.08349   Accuracy: 0.978\n","Train   Epoch: 008 / 020   Loss: 0.02533   Accuracy: 0.992\n"," Test   Epoch: 008 / 020   Loss: 0.08237   Accuracy: 0.977\n","Train   Epoch: 009 / 020   Loss: 0.02274   Accuracy: 0.993\n"," Test   Epoch: 009 / 020   Loss: 0.09285   Accuracy: 0.973\n","Train   Epoch: 010 / 020   Loss: 0.01954   Accuracy: 0.994\n"," Test   Epoch: 010 / 020   Loss:   0.134   Accuracy: 0.967\n","Train   Epoch: 011 / 020   Loss:  0.0167   Accuracy: 0.995\n"," Test   Epoch: 011 / 020   Loss: 0.08632   Accuracy: 0.979\n","Train   Epoch: 012 / 020   Loss:  0.0128   Accuracy: 0.996\n"," Test   Epoch: 012 / 020   Loss: 0.09646   Accuracy: 0.977\n","Train   Epoch: 013 / 020   Loss: 0.009427   Accuracy: 0.997\n"," Test   Epoch: 013 / 020   Loss:  0.1074   Accuracy: 0.975\n","Train   Epoch: 014 / 020   Loss: 0.008406   Accuracy: 0.998\n"," Test   Epoch: 014 / 020   Loss: 0.09693   Accuracy: 0.977\n","Train   Epoch: 015 / 020   Loss: 0.004458   Accuracy: 0.999\n"," Test   Epoch: 015 / 020   Loss: 0.09116   Accuracy: 0.980\n","Train   Epoch: 016 / 020   Loss: 0.003318   Accuracy: 0.999\n"," Test   Epoch: 016 / 020   Loss:   0.096   Accuracy: 0.980\n","Train   Epoch: 017 / 020   Loss: 0.001694   Accuracy: 1.000\n"," Test   Epoch: 017 / 020   Loss: 0.09291   Accuracy: 0.982\n","Train   Epoch: 018 / 020   Loss: 0.001114   Accuracy: 1.000\n"," Test   Epoch: 018 / 020   Loss: 0.09175   Accuracy: 0.981\n","Train   Epoch: 019 / 020   Loss: 0.0009481   Accuracy: 1.000\n"," Test   Epoch: 019 / 020   Loss: 0.09252   Accuracy: 0.982\n","Train   Epoch: 020 / 020   Loss: 0.000819   Accuracy: 1.000\n"," Test   Epoch: 020 / 020   Loss: 0.09273   Accuracy: 0.982\n"]}]},{"cell_type":"markdown","metadata":{"id":"bxim_oNddkqE"},"source":["# Submit Your Solution"]},{"cell_type":"code","metadata":{"cellView":"form","id":"5OjcF__gXmth","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1638011488485,"user_tz":-120,"elapsed":1752,"user":{"displayName":"Guy Lutsker","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428063293512509787"}},"outputId":"a907a29d-21c3-448b-c90c-3cc7d51aee20"},"source":["#@title # Create and Download Your Solution\n","\n","import os\n","import re\n","import zipfile\n","from google.colab import files\n","\n","def create_zip(files, hw, name):\n","  zip_path = f'{hw}-{name}.zip'\n","  with zipfile.ZipFile(zip_path, 'w') as f:\n","    for fname in files:\n","      if not os.path.isfile(fname):\n","        raise FileNotFoundError(f\"Couldn't find file: '{fname}' in the homework directory\")\n","      f.write(fname, fname)\n","  return zip_path\n","\n","# export notebook as html\n","!jupyter nbconvert --to html hw2.ipynb\n","\n","##@markdown Please upload your typed solution (`.pdf` file) to the homework directory, and use the name `hw2-sol.pdf`.\n","\n","student_name = \"Guy Lutsker\"  #@param{type:\"string\"}\n","assignment_name = 'hw2'\n","assignment_sol_files = ['hw2.ipynb', 'hw2.html', 'autograd.py', 'functional.py', 'nn.py', 'optim.py',\n","                        'models.py', 'models_torch.py', 'train.py', 'train_torch.py']\n","zip_name = re.sub('[_ ]+', '_', re.sub(r'[^a-zA-Z_ ]+', '', student_name.lower()))\n","\n","# create zip with your solution\n","zip_path = create_zip(assignment_sol_files, assignment_name, zip_name)\n","\n","# download the zip\n","files.download(zip_path)\n","\n","#@markdown Enter your name in `student_name` and run this cell to create and download a `.zip` file with your solution.\n","\n","#@markdown You should submit your solution via the submission link in the Moodle.\n","\n","#@markdown **Note:** If you run this cell multiple times, you may be prompted by the browser to allow this page to download multiple files."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook hw2.ipynb to html\n","[NbConvertApp] Writing 361783 bytes to hw2.html\n"]},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_5b6bd244-1cb3-4933-a40e-8ef9b4ad5cb1\", \"hw2-guy_lutsker.zip\", 451366)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}}]}]}